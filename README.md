# paas-data-ingestion
Ingest and prepare data with AWS lambdas, Snowflake and dbt in a scalable, fully replayable manner.

## Overview
This repository contains a fully PaaS infrastructure for data ingestion and transformation at scale. This repository has a companion blog post (forthcoming), to which we refer the reader for in-depth analysis of the proposed patterns and the motivations behind the project.

The ingestion pipeline mimics a typical data flow for data-driven applications: clients send events, an endpoint collects them and dumps them into a stream, finally a data warehouse stores them for further processing:

<img src="https://github.com/jacopotagliabue/paas-data-ingestion/blob/main/images/data_pattern_viz.jpg" width="640">

We make use of three main technologies:

* [Pulumi](https://www.pulumi.com/), which allows us to manipulate infrastructure-as-code, and to do so in a language we are very familiar with, Python (this is our first project using it coming from Terraform, so any feedback is very welcome!).
* [Snowflake](https://signup.snowflake.com/), which allows us to store raw data at scale and manipulate it with powerful SQL queries, abstracting away all the complexity of distributed computing in a simple API.
* [dbt](https://www.getdbt.com/), which allows us to define data transformation as versioned, replayable DAGs, and mix-and-match materialization strategies to suite our needs.

Finally, it is worth mentioning that the repository is e-commerce related as a results of mainly three factors:

* our own experience in building [pipelines for the industry](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat);
* the availability of high-volume, [high-quality data](https://github.com/coveooss/SIGIR-ecom-data-challenge) to simulate a realistic scenario;
* finally, the high bar set by e-commerce as far as data quantity and data-driven applications: due to the nature of the business and the competition, even medium-sized digital shops tend to produce an enormous amount of data, and to run pretty sophisticated ML flows on top it - in other words, something that works for e-commerce is going to reasonably work for many other use cases out of the box, as they are likely less data intensive and less sophisticated ML-wise.

Our goal was to keep the code realistic enough for the target use cases, but simple enough as to make it easy for everybody to port this stack to a different industry.

## Prerequisites

* _Dataset_: we use the [Coveo Data Challenge dataset](https://github.com/coveooss/SIGIR-ecom-data-challenge) as our real-world dataset (if you prefer to use your own data, you can still re-use the `pumper.py` app if you adapt the format to your input files);
* _Snowflake account_: is the warehouse where we will save all the events collected by the service and the final tables generated by DBT. [Sign-up for a 30-day free trial](https://signup.snowflake.com).
* _Pulumi account_: is the service we use to sucirely persist the state of the infrastructure. [Sign-up for a free Individual account](https://www.pulumi.com).
* _AWS account_: is the cloud provider we use to host the lambda function (the `/collect` endpoint), to collect (Firehose) and store (S3) the logs and to sent the final gzipped logs into Snowflake (Snowpipe). [Sign-up for a free AWS account](https://aws.amazon.com/free/) (S3, API Gateway & Lambda have a free tier).

TBC

## Structure

The project is divided in three main components and a simulation script.

### Infrastructure

We used Pulimi to define the whole infrastructure (for managing AWS & Snowflake resources) via code.

Pulimi uses the `infrastructure/__main__.py` file to check which services have to create/updated or removed. The script defines:

- `s3_logs_bucket` is the S3 bucket we use to store all the logs sent to Firehose
- `sf_database` is the database Pulumi will create on your Snowflake account
- `sf_warehouse` is the warehouse (computing) we will use to execute all the SQL queries
- `sf_roles` is a custom module we built `infrastructure/my_snowflake_roles.py` to create and manage 2 different roles:
  - a Read-Write `*_RW` profile, usually reserved to the service's administrators
  - a Read-Only `*_RO` profile
- `sf_stream_schema` is the main schema where all the RAW logs will be stored
- `sf_stream_logs_snowpipe` is a custom module we built `infrastructure/my_snowflake_snowpipe.py` to configure Snowpipe and all the policies required to allow the Snowflake's AWS account to read the data stored on your personal S3 bucket
  - `self.firehose` is the [Firehose](https://aws.amazon.com/it/kinesis/data-firehose/) stream where all the RAW logs will be sent by the Lambda Function
  - `self.table` is the final table where all the RAW logs will be stored by Snowpipe
  - `self.storage_integration`, `self.stage` & `self.pipe` define the Snowflake resource required to allow Snowpipe to read your data
  - finally we use `aws.s3.BucketNotification` to notify Snowpipe every time a new object (file) is stored (by Firehose) on your S3 Bucket
- `api` & `api_stage` define [API Gateway](https://aws.amazon.com/it/api-gateway/), a public entry-point we'll use to expose our Lambda function.
- `lambda_api_collect` is a custom module we build `infrastructure/my_lambda.py` to define the Lambda function for the `/collect` endpoint.

### dbt

We used DBT to process the RAW logs and to normalize the data into 3 different schemes:

- `EVENTS`, stored on `dbt/models/events/marts`, contains all the materialized tables (`events`, `logs`, `pageviews`, `product_actions`, `purchases` & `user_agents`) with the data computed during the last execution of DBT;
- `EVENTS_LIVE`, stored on `dbt/models/events/live`, contains the same tables above (`events`, `logs` etc...) updated in real-time; the views are the result of the union of the materialized table and all the new events that DBT have still to process;
- `STATS`, stored on `dbt/models/stats`, defines materialized tables (`overview_1h`, `overview_1d`) containing all the main pre-calculated stats a dashboard can use (`pageviews`, `add_to_cart`, `transactions`, `transaction_revenue` etc).

We decided to define the SQL queries in macros because the tables stored on the `EVENTS` & `EVENTS_LIVE` schemes are the same but contain data from different analysis periods.

For example, the `logs` table containing all the sessionized and enriched logs used as the root for all the other tables (`events`, `logs` etc...):

- `dbt/models/marts/logs.sql` is the materialized version of the table (`EVENTS.LOGS`); we use an incremental materialization and the SQL query is defined on the `dbt/macros/events/select_logs.sql` table
- `dbt/models/live/logs_live.sql` is the live version of the table (`EVENTS_LIVE.LOGS`); the script does a UNION between all the events stored on `EVENTS.LOGS` with all the new events defined on the `EVENTS.LOGS_STAGED` table (`dbt/models/live/logs_staged.sql`)

Both the `EVENTS.LOGS` and the `EVENTS_LIVE.LOGS_STAGED` table uses the same macro `dbt/macros/events/select_logs.sql` but with different filters.

Finally, for demonstration purposes, we have integrated the [UAParser.js](https://github.com/faisalman/ua-parser-js/tree/master/dist) library for parsing the user-agents directly on Snowflake.


### AWS lambda

A simple AWS lambda function implementing a [data collection pixel](https://medium.com/tooso/serving-1x1-pixels-from-aws-lambda-endpoints-9eff73fe7631). Once deployed, the resulting `/collect` endpoint will accept `POST` requests from clients sending e-commerce data: the function is kept simple for pedagogical reason - after accepting the body, it prepares a simple but structured event, and uses another AWS PaaS service, Firehose, to dump it in a stream for downstream storage and further processing.

### Data pumper

To simulate a constant stream of events reaching the collect endpoint, we provide a script that can be run at will to upload e-commerce events in the [Google Analytics](https://developers.google.com/analytics/devguides/collection/protocol/v1/parameters) format.

The events are based on the real-world clickstream dataset open sourced in 2021, the [Coveo Data Challenge](https://github.com/coveooss/SIGIR-ecom-data-challenge) dataset. By using real-world anonymized events we provide practitioners with a realistic, non-toy scenario to get acquainted with the design patterns we propose. Please cite our work, share / star the repo if you find the dataset useful!

## How to run it

Running the stack involves running three operations:

* setting up the infrastructure;
* send data;
* run dbt transformations.

### Setting up the infrastructure

1. Jump into the project folder
    ```sh
    cd infrastructure
    ```
2. [Install Pulumi](https://www.pulumi.com/docs/get-started/install/) on your computer and configure your [Pulumi account](https://www.pulumi.com/docs/reference/cli/pulumi_login/):
    ```sh
    pulumi login
    ```
3. Setup the Python venv
    ```sh
    make init
    ```
4. Create a new Pulumi Stack:
    ```sh
    pulumi stack init dev
    ```
5. Configure the new stack with all the required credentials:
    ```sh
    # AWS
    pulumi config set aws:region <value>
    pulumi config set aws:accessKey <value>
    pulumi config set aws:secretKey <value> --secret
    # Snowflake
    pulumi config set snowflake:region <value>
    pulumi config set snowflake:account <value>
    pulumi config set snowflake:password <value> --secret
    pulumi config set snowflake:username <value>
    ```
    All the configurations will be stored on the `Pulumi.dev.yaml` file.
6. Deploy the stack:
    ```sh
    make up
    ```

Notes:
- Make sure to use the [AWS credentiales with Administrator permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html);
- Use the same region for AWS and Snowflake (we suggest you to use `us-east-1`);
- Follow [this guide](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html) to identify your Snowflake account & region name.


### Send data with the pumper

* To pump data into the newly created stack, `cd` into the `pumper` folder, create a virtual environment and activate it, then install the requirements with `pip install -r requirements.txt`.
* Download and unzip the [Coveo Data Challenge dataset](https://github.com/coveooss/SIGIR-ecom-data-challenge) into a local folder and write down the full path, e.g. `myfolder/train`
* Create a copy of `.env.local` named `.env`, and use the dataset path as the value for `DATA_FOLDER`, the AWS url for the lambda function for `COLLECT_URL`.
* Run `python pumper.py` to start sending Google Analytics events to the endpoint. The script will run until `N_EVENTS` has been sent (change the variable in the script to what you like).

At every run, `pumper.py` will send events as they are happening in that very moments: so running the code two times will not produce duplicate events, but events with similar categorical features and different id, timestamp etc.

Please note that if you want to jump start the log table by bulk-loading the dataset (or a portion of it) to Snowflake, you can avoid some idle time waiting for events to be sent by using the [copy into](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html) function over the raw csv.

## Bonus: mix and match materialization options

TBC

## Contributors

This project has been brought to you with love by:

* [Luca Bigon](https://www.linkedin.com/in/bigluck/): design, infrastructure, SQL-ing
* [Jacopo Tagliabue](https://www.linkedin.com/in/jacopotagliabue/): design, data

## License
The code is provided "as is" and released under an open MIT License.
